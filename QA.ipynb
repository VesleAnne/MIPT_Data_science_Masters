{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1qyGN3T-bFZgYpKzklv0dUJUe34fpLseA","authorship_tag":"ABX9TyOOpdr6WP8NYvyAQ/5CFP6e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9d0d512b953f4677af35f12e1e65812b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_797c53588a1b4c4aa127b4f209f8c595","IPY_MODEL_c15b59f6ba524c2a8006d7edb6a0b7b2","IPY_MODEL_1760305bde264bd28a457bdaf407f291"],"layout":"IPY_MODEL_c6cbfe49f5fd4b6a9627e1ee4c9a56bb"}},"797c53588a1b4c4aa127b4f209f8c595":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7afa3b6177a47389afe650790a14048","placeholder":"​","style":"IPY_MODEL_c9e82efbcfbf4e0e8403fc7be2dad569","value":"Map: 100%"}},"c15b59f6ba524c2a8006d7edb6a0b7b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6126ca7bb2749aca4faec07082b79e3","max":45328,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70334d834250427f8e9e6ec479f46e82","value":45328}},"1760305bde264bd28a457bdaf407f291":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14e1d738bbba428ba9ce2f8051eee6de","placeholder":"​","style":"IPY_MODEL_aed7915b3e49406ca382391da62173d0","value":" 45328/45328 [01:40&lt;00:00, 533.35 examples/s]"}},"c6cbfe49f5fd4b6a9627e1ee4c9a56bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7afa3b6177a47389afe650790a14048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9e82efbcfbf4e0e8403fc7be2dad569":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6126ca7bb2749aca4faec07082b79e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70334d834250427f8e9e6ec479f46e82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14e1d738bbba428ba9ce2f8051eee6de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aed7915b3e49406ca382391da62173d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e53fff2154b4140ab5b40a5bca37686":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_456f9816913f4a40808101a4f9fc902d","IPY_MODEL_11bf6fe698564226be72794cf99f7e84","IPY_MODEL_0d3ea2fab9a3489e8880b2d8448f50c3"],"layout":"IPY_MODEL_87f77896da764a3b87fceb2ab412b840"}},"456f9816913f4a40808101a4f9fc902d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dc0a04422da4944a0fa26be8aa41f65","placeholder":"​","style":"IPY_MODEL_6f705be237f54bc29e7814d6eaf1d5b1","value":"Map: 100%"}},"11bf6fe698564226be72794cf99f7e84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f68dec3a820402f9fea762a9d8fbb79","max":5036,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b18e45774acf46d19b25db98734199c1","value":5036}},"0d3ea2fab9a3489e8880b2d8448f50c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ce504d5779046adacd50cb1bce723e8","placeholder":"​","style":"IPY_MODEL_6f691cf74b5c480fae1b849fda8f123b","value":" 5036/5036 [00:09&lt;00:00, 627.20 examples/s]"}},"87f77896da764a3b87fceb2ab412b840":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dc0a04422da4944a0fa26be8aa41f65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f705be237f54bc29e7814d6eaf1d5b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f68dec3a820402f9fea762a9d8fbb79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b18e45774acf46d19b25db98734199c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ce504d5779046adacd50cb1bce723e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f691cf74b5c480fae1b849fda8f123b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17ce8c20e4fd40b083543d4d6a3a2271":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd59e19cc8ab49eeb1c92bee5064b1b7","IPY_MODEL_fa6add9b05a94bc29bb6a243a6fee714","IPY_MODEL_e6d8236a95d34573852bab2da3adfeeb"],"layout":"IPY_MODEL_08d807480e454a51b0e301a1ac5c3498"}},"bd59e19cc8ab49eeb1c92bee5064b1b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ea370145ad943648dc6c54ce5acab3d","placeholder":"​","style":"IPY_MODEL_2b27152e901d476199919b32395aeb5d","value":"Map: 100%"}},"fa6add9b05a94bc29bb6a243a6fee714":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64a874416f5e4ce6b02fa3e5ed63fd61","max":23936,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b768db1678e41169b99a957ea8617ab","value":23936}},"e6d8236a95d34573852bab2da3adfeeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9879c07576c4d3a8c53a38964a80684","placeholder":"​","style":"IPY_MODEL_c421ad055acb4172952cbf6859afdf80","value":" 23936/23936 [00:43&lt;00:00, 466.14 examples/s]"}},"08d807480e454a51b0e301a1ac5c3498":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ea370145ad943648dc6c54ce5acab3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b27152e901d476199919b32395aeb5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64a874416f5e4ce6b02fa3e5ed63fd61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b768db1678e41169b99a957ea8617ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9879c07576c4d3a8c53a38964a80684":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c421ad055acb4172952cbf6859afdf80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62c2a7432a00483f9f8440f9a82a1911":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_1592d67a3d0546f59747678831fab1eb","IPY_MODEL_1b22b8ca1a444fc08332abe291bdb93b","IPY_MODEL_a66dcda28d7b42f5831ab5516c5bdc0c","IPY_MODEL_ba2e64f41af541038c7fd58a7740781f","IPY_MODEL_b5dfc0604f1846e29afdfe72a5fcbef4"],"layout":"IPY_MODEL_82629098488f497ba57c3044a477e2bb"}},"1592d67a3d0546f59747678831fab1eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa1c3bba3260420f9597dd794c58cca4","placeholder":"​","style":"IPY_MODEL_2625efce185a48c9a91b688dfcea9551","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"1b22b8ca1a444fc08332abe291bdb93b":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_d0a8f9b6d41d4f589d31403febd38000","placeholder":"​","style":"IPY_MODEL_58ad858c03f64deb82517b029b11ffc5","value":"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"}},"a66dcda28d7b42f5831ab5516c5bdc0c":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_6f219e4ba8504e43b8d39fad0d62e168","style":"IPY_MODEL_5ed840f617254818a82b5aea231a90aa","value":true}},"ba2e64f41af541038c7fd58a7740781f":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_a7128d02bb6a4287823a47e6b0de83f1","style":"IPY_MODEL_1605fde3bc3b41dfa6924bb350473335","tooltip":""}},"b5dfc0604f1846e29afdfe72a5fcbef4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbbae6f0e0b14f25b2af00b3deed174c","placeholder":"​","style":"IPY_MODEL_cbde94fc455643e788834bdcaddbf14b","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"82629098488f497ba57c3044a477e2bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"fa1c3bba3260420f9597dd794c58cca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2625efce185a48c9a91b688dfcea9551":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0a8f9b6d41d4f589d31403febd38000":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ad858c03f64deb82517b029b11ffc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f219e4ba8504e43b8d39fad0d62e168":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ed840f617254818a82b5aea231a90aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7128d02bb6a4287823a47e6b0de83f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1605fde3bc3b41dfa6924bb350473335":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"cbbae6f0e0b14f25b2af00b3deed174c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbde94fc455643e788834bdcaddbf14b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Для закрепления материала модуля предлагаем вам решить задачу QA для датасета SberQuad, используя любые доступные вам средства.\n","\n","Для достижения наилучшего результата уделите внимание подбору гиперапарметров как в плане архитектуры, так и в плане обучения модели.\n","\n","Критерии оценивания проекта:\n","\n","общее качество кода и следование PEP-8;\n","\n","\n","* использование рекуррентных сетей;\n","* использованы варианты архитектур, близкие к state of the art для данной\n","* задачи;\n","* произведен подбор гиперпараметров;\n","* использованы техники изменения learning rate (lr scheduler);\n","* использована адекватная задаче функция потерь;\n","* использованы техники регуляризации;\n","* корректно проведена валидация модели;\n","* использованы техники ensemble;\n","* использованы дополнительные данные;\n","* итоговое значение метрики качества > 0.75 (f1).\n","\n","---\n","\n","\n","Выполните задание в Google Colab и в поле для ответа ниже вставьте ссылку на ваше решение. Не забудьте открыть доступ!\n","\n"],"metadata":{"id":"xU9mM0NfsgQt"}},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wije-J1TaGG","executionInfo":{"status":"ok","timestamp":1697539545314,"user_tz":-180,"elapsed":8531,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"3ae468f2-30fa-47ee-96fb-42f1c62b201d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]}]},{"cell_type":"code","source":["!pip install transformers[torch]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5VZPmJNTwGL","executionInfo":{"status":"ok","timestamp":1697539552916,"user_tz":-180,"elapsed":7653,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"5987764b-af5b-4415-c585-f990809acde7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (17.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"]}]},{"cell_type":"code","source":["import datasets"],"metadata":{"id":"q-g0REujS9ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer,  AutoModelForQuestionAnswering, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\n","import torch\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn as nn\n","import torch.optim as optim\n","from datasets import DatasetDict\n","from sklearn.metrics import f1_score\n"],"metadata":{"id":"vElo4XtXXeHK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Определяем устройство (CPU или GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"atignRMvwDNU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRoeMHursY64"},"outputs":[],"source":["dataset = load_dataset(\"sberquad\")"]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-XB4Cc6V1FT","executionInfo":{"status":"ok","timestamp":1697539554414,"user_tz":-180,"elapsed":11,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"cfa652c4-8f43-481f-ebff-f2eb6c2b3202"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 45328\n","    })\n","    validation: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 5036\n","    })\n","    test: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 23936\n","    })\n","})"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["dataset['train']['question'][15]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"fr51mgxcgJj0","executionInfo":{"status":"ok","timestamp":1697539555208,"user_tz":-180,"elapsed":801,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"502e70f2-2d63-46b0-b0e9-681b6d3bf0a6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'У кого Россия арендует этот комплекс?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["dataset['train']['answers'][15]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Zwo1d7JgPhT","executionInfo":{"status":"ok","timestamp":1697539556838,"user_tz":-180,"elapsed":1646,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"e67b8529-797c-4630-e13f-2e2c14fe5775"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': ['у Казахстана'], 'answer_start': [93]}"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["dataset['train']['context'][15]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"BOd4wXoGWCyH","executionInfo":{"status":"ok","timestamp":1697539557647,"user_tz":-180,"elapsed":812,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"ad56c3d8-0b33-4425-db60-7111172a4006"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Город Байконур и космодром Байконур вместе образуют комплекс Байконур , арендованный Россией у Казахстана на период до 2050 года. Эксплуатация космодрома стоит около 9 млрд рублей в год (стоимость аренды комплекса Байконур составляет 115 млн долларов — около 7,4 млрд рублей в год; ещё около 1,5 млрд рублей в год Россия тратит на поддержание объектов космодрома), что составляет 4,2 % от общего бюджета Роскосмоса на 2012 год. Кроме того, из федерального бюджета России в бюджет города Байконура ежегодно осуществляется безвозмездное поступление в размере 1,16 млрд рублей (по состоянию на 2012 год). В общей сложности космодром и город обходятся бюджету России в 10,16 млрд рублей в год.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["model_name = \"timpal0l/mdeberta-v3-base-squad2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xXP38k3i4zy","executionInfo":{"status":"ok","timestamp":1697539558820,"user_tz":-180,"elapsed":1181,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"cda1912b-94ea-4b82-8fb2-cdc4942595d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["max_length = 384\n","stride = 128\n"],"metadata":{"id":"5aCAg091Ok50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Функция для преобразования тренировочных данных\n","\n","\n","\n","def preprocess_training_examples(examples):\n","    questions = [q.strip() for q in examples['question']]\n","    inputs = tokenizer(\n","        questions,\n","        examples['context'],\n","        max_length=max_length,\n","        truncation='only_second',\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding='max_length',\n","        )\n","\n","    offset_mapping = inputs.pop('offset_mapping')\n","    sample_map = inputs.pop('overflow_to_sample_mapping')\n","    answers = examples['answers']\n","    start_positions = []\n","    end_positions = []\n","\n","    for (i, offset) in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer['answer_start'][0]\n","        end_char = answer['answer_start'][0] + len(answer['text'][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","    # Find the start and end of the context\n","\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","    # If the answer is not fully inside the context, label is (0, 0)\n","\n","        if offset[context_start][0] > end_char \\\n","            or offset[context_end][1] < start_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","\n","      # Otherwise it's the start and end token positions\n","\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs['start_positions'] = start_positions\n","    inputs['end_positions'] = end_positions\n","    return inputs"],"metadata":{"id":"Xg7xTXTdORGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = dataset[\"train\"].map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=dataset[\"train\"].column_names,\n",")"],"metadata":{"id":"zk5an8IqOapL","executionInfo":{"status":"ok","timestamp":1697539658795,"user_tz":-180,"elapsed":99989,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9d0d512b953f4677af35f12e1e65812b","797c53588a1b4c4aa127b4f209f8c595","c15b59f6ba524c2a8006d7edb6a0b7b2","1760305bde264bd28a457bdaf407f291","c6cbfe49f5fd4b6a9627e1ee4c9a56bb","f7afa3b6177a47389afe650790a14048","c9e82efbcfbf4e0e8403fc7be2dad569","b6126ca7bb2749aca4faec07082b79e3","70334d834250427f8e9e6ec479f46e82","14e1d738bbba428ba9ce2f8051eee6de","aed7915b3e49406ca382391da62173d0"]},"outputId":"da1febb5-4cc8-40a6-e19d-f146cae603f5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/45328 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d0d512b953f4677af35f12e1e65812b"}},"metadata":{}}]},{"cell_type":"code","source":["val_dataset = dataset[\"validation\"].map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=dataset[\"validation\"].column_names,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["3e53fff2154b4140ab5b40a5bca37686","456f9816913f4a40808101a4f9fc902d","11bf6fe698564226be72794cf99f7e84","0d3ea2fab9a3489e8880b2d8448f50c3","87f77896da764a3b87fceb2ab412b840","7dc0a04422da4944a0fa26be8aa41f65","6f705be237f54bc29e7814d6eaf1d5b1","6f68dec3a820402f9fea762a9d8fbb79","b18e45774acf46d19b25db98734199c1","5ce504d5779046adacd50cb1bce723e8","6f691cf74b5c480fae1b849fda8f123b"]},"id":"bGy8WXq2Pmea","executionInfo":{"status":"ok","timestamp":1697539667704,"user_tz":-180,"elapsed":8966,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"e154728d-171d-4fbc-dd3b-b7dca07a3734"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5036 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e53fff2154b4140ab5b40a5bca37686"}},"metadata":{}}]},{"cell_type":"code","source":["# Функция для преобразования тестовых данных\n","\n","def preprocess_test_examples(examples):\n","    questions = [q.strip() for q in examples['question']]\n","    inputs = tokenizer(\n","        questions,\n","        examples['context'],\n","        max_length=max_length,\n","        truncation='only_second',\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding='max_length',\n","        )\n","\n","    sample_map = inputs.pop('overflow_to_sample_mapping')\n","    example_ids = []\n","\n","    for i in range(len(inputs['input_ids'])):\n","        sample_idx = sample_map[i]\n","        example_ids.append(examples['id'][sample_idx])\n","\n","        sequence_ids = inputs.sequence_ids(i)\n","        offset = inputs['offset_mapping'][i]\n","        inputs['offset_mapping'][i] = [(o if sequence_ids[k]\n","                == 1 else None) for (k, o) in enumerate(offset)]\n","\n","    inputs['example_id'] = example_ids\n","    return inputs"],"metadata":{"id":"sJUIr72rO2Su"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = dataset[\"test\"].map(\n","    preprocess_test_examples,\n","    batched=True,\n","    remove_columns=dataset[\"test\"].column_names,\n",")"],"metadata":{"id":"d7aof2OyO2WK","executionInfo":{"status":"ok","timestamp":1697539712089,"user_tz":-180,"elapsed":44390,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["17ce8c20e4fd40b083543d4d6a3a2271","bd59e19cc8ab49eeb1c92bee5064b1b7","fa6add9b05a94bc29bb6a243a6fee714","e6d8236a95d34573852bab2da3adfeeb","08d807480e454a51b0e301a1ac5c3498","7ea370145ad943648dc6c54ce5acab3d","2b27152e901d476199919b32395aeb5d","64a874416f5e4ce6b02fa3e5ed63fd61","8b768db1678e41169b99a957ea8617ab","f9879c07576c4d3a8c53a38964a80684","c421ad055acb4172952cbf6859afdf80"]},"outputId":"f567c964-7a36-4bd8-a8a6-24de392eb9f8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/23936 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ce8c20e4fd40b083543d4d6a3a2271"}},"metadata":{}}]},{"cell_type":"code","source":["def compute_f1(predictions, label_ids):\n","    def normalize_answer(s):\n","        \"\"\"Нормализация текстового ответа\"\"\"\n","        return \" \".join(s.strip().split())\n","\n","    def get_tokens(s):\n","        \"\"\"Разбивка текста на токены\"\"\"\n","        if not s:\n","            return []\n","        return normalize_answer(s).split()\n","\n","    f1_scores = []\n","    for pred, true in zip(predictions, label_ids):\n","        pred_start, pred_end = pred\n","        true_start, true_end = true\n","\n","        # Получение токенов для предсказанных и истинных ответов\n","        pred_tokens = get_tokens(pred_start) + get_tokens(pred_end)\n","        true_tokens = get_tokens(true_start) + get_tokens(true_end)\n","\n","        common = collections.Counter(true_tokens) & collections.Counter(pred_tokens)\n","        num_same = sum(common.values())\n","\n","        if num_same == 0:\n","            f1_scores.append(0)\n","            continue\n","\n","        precision = 1.0 * num_same / len(pred_tokens)\n","        recall = 1.0 * num_same / len(true_tokens)\n","\n","        f1 = (2 * precision * recall) / (precision + recall)\n","        f1_scores.append(f1)\n","\n","    return sum(f1_scores) / len(f1_scores)  # Среднее значение F1 для всех примеров"],"metadata":{"id":"lt6oKvUr3Pc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Определим функцию для вычисления метрики\n","#def compute_metrics(eval_predictions):\n","#    f1_scores = []\n","\n","#    for prediction, reference in zip(eval_predictions, dataset[\"validation\"]):\n","#        predicted_answer = prediction[\"predicted_text\"]\n","#        true_answer = reference[\"answers\"][\"text\"]\n","\n","#        common_tokens = set(predicted_answer.lower().split()).intersection(set(true_answer.lower().split()))\n","#        precision = len(common_tokens) / (len(predictated_answer.split()) + 1e-8)\n","#        recall = len(common_tokens) / (len(true_answer.split()) + 1e-8)\n","\n","#        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n","#        f1_scores.append(f1)\n","\n","#    return {\"f1\": np.mean(f1_scores)}\n"],"metadata":{"id":"xvs-MrtAtkIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# подключимся к аккаунту\n","from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331,"referenced_widgets":["62c2a7432a00483f9f8440f9a82a1911","1592d67a3d0546f59747678831fab1eb","1b22b8ca1a444fc08332abe291bdb93b","a66dcda28d7b42f5831ab5516c5bdc0c","ba2e64f41af541038c7fd58a7740781f","b5dfc0604f1846e29afdfe72a5fcbef4","82629098488f497ba57c3044a477e2bb","fa1c3bba3260420f9597dd794c58cca4","2625efce185a48c9a91b688dfcea9551","d0a8f9b6d41d4f589d31403febd38000","58ad858c03f64deb82517b029b11ffc5","6f219e4ba8504e43b8d39fad0d62e168","5ed840f617254818a82b5aea231a90aa","a7128d02bb6a4287823a47e6b0de83f1","1605fde3bc3b41dfa6924bb350473335","cbbae6f0e0b14f25b2af00b3deed174c","cbde94fc455643e788834bdcaddbf14b"]},"id":"PihobnFDQ5ZI","executionInfo":{"status":"ok","timestamp":1697539712113,"user_tz":-180,"elapsed":60,"user":{"displayName":"Anna Simonova","userId":"09473448747889040145"}},"outputId":"93de7e3c-cfda-4a4b-f6e5-67a22045e63b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62c2a7432a00483f9f8440f9a82a1911"}},"metadata":{}}]},{"cell_type":"code","source":["model = AutoModelForQuestionAnswering.from_pretrained(model_name)"],"metadata":{"id":"aIi5dvv89EwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Определим, какие слои модели будем обучать (последние 4 слоя)\n","for param in model.base_model.parameters():\n","    param.requires_grad = False\n","for param in model.base_model.encoder.layer[-4:].parameters():\n","    param.requires_grad = True"],"metadata":{"id":"rs7gArH-9EtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Определим гиперпараметры\n","learning_rate = 2e-5\n","epochs = 3"],"metadata":{"id":"-yHCwqJJ9EpW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Определим функцию потерь и оптимизатор\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"],"metadata":{"id":"E7FkZZ599EmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"81Fgn3ACBVwF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Создаем объект EarlyStoppingCallback для остановки обучения\n","# Я знаю, что при обучении на трех эпохах этот шаг не имеет смысла, но мне просто не хватило ресурсов колаба, чтобы обучиться на большем количестве эпох\n","early_stopping = EarlyStoppingCallback(early_stopping_patience=3)"],"metadata":{"id":"lncjISK19Ein"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Определяем TrainingArguments\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    output_dir='./content/drive/MyDrive/МФТИ/Question answering',  # Путь для сохранения модели и результатов\n","    num_train_epochs=epochs,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=500,  # Оценивать модель каждые 500 шагов\n","    save_total_limit=3,  # Максимальное количество сохранений модели\n","    load_best_model_at_end=True,\n","    learning_rate=learning_rate,\n","    remove_unused_columns=False,\n","    report_to=\"tensorboard\",\n","    push_to_hub=False\n",")"],"metadata":{"id":"fH0JBURh_Ld1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Создаем объект Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    #compute_metrics=compute_metrics\n",")"],"metadata":{"id":"FRK8WGCI_Yl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Обучение\n","trainer.add_callback(early_stopping)\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113},"id":"5DHC-M-Y_Ypu","outputId":"36f46c3f-0de6-4c4a-8f50-1a5904b950eb"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='36' max='4473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  36/4473 1:21:35 < 177:28:37, 0.01 it/s, Epoch 0.02/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":["model.push_to_hub(\"VesleAnne/sberquad_mdeberta\")"],"metadata":{"id":"HZWkdP2-RHy6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Загружаем  модель\n","model = AutoModelForQuestionAnswering.from_pretrained('VesleAnne/sberquad_mdeberta')\n"],"metadata":{"id":"Yb_lk68s_lSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Оцениваем модель на тестовом наборе данных\n","results = trainer.predict(test_dataset)\n","f1_score = compute_f1(results.predictions, results.label_ids)\n","print(\"F1 Score on Test Data:\", f1_score)"],"metadata":{"id":"GizQzdg-_lWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, dataset):\n","  f1_scores = []\n","\n","  dataloader = DataLoader(dataset=list(\n","      zip(\n","            dataset['input_ids'],\n","            dataset['attention_mask'],\n","            dataset['start_positions'],\n","            dataset['end_positions']\n","            )\n","      ), batch_size=16\n","  )\n","\n","  for input_ids, attention_mask, start_positions, end_positions in dataloader:\n","    input_ids = torch.stack(input_ids).T.to(model.device)\n","    attention_mask = torch.stack(attention_mask).T.to(model.device)\n","    start_positions, end_positions = start_positions.to(model.device), end_positions.to(model.device)\n","    logits = model(input_ids, attention_mask)\n","\n","    pred_start_positions = logits.start_logits.argmax(1)\n","    pred_end_positions = logits.end_logits.argmax(1)\n","\n","    true_seqs = np.zeros_like(input_ids.detach().cpu())\n","    pred_seqs = np.zeros_like(input_ids.detach().cpu())\n","\n","    for i in range(true_seqs.shape[0]):\n","      true_seqs[i][start_positions[i]:end_positions[i]+1] = 1\n","      pred_seqs[i][pred_start_positions[i]:pred_end_positions[i]+1] = 1\n","\n","    f1_score_mean = np.mean([f1_score(\n","        true_seqs[i], pred_seqs[i]) for i in range(len(true_seqs))]\n","    )\n","    f1_scores.append(f1_score_mean)\n","  return np.mean(f1_scores)"],"metadata":{"id":"pRmyfswLQWHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C_jFrMpa_lbe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6pHHpPnZ9Eb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["раз"],"metadata":{"id":"GYuYn62S9EWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8GMfvfoI9ETK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q-7Ua2mt2Ck3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Yp21OU5v2CoN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 356\n","\n","def preprocess_data(examples):\n","    questions = [q.strip() for q in examples[\"question\"]] #инициализация вопросов\n","    contexts = examples[\"context\"]\n","\n","    # токенезируем вопросы и контексты\n","    inputs = tokenizer(\n","        questions,\n","        contexts,\n","        max_length=max_length,\n","        truncation=True,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # извлечение данных\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Находим начало и конец контекста\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # Если ответ не полностью в контексте, то меткой будет (0, 0)\n","        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    # обновим входные данные\n","    inputs['input_ids'] = inputs['input_ids']\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs"],"metadata":{"id":"dbLCUIYKjc-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_dataset(examples):\n","    return preprocess_data(examples)"],"metadata":{"id":"ytgvduxMjdDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_qa_data(dataset, tokenizer, max_length=356):\n","    input_ids = []\n","    attention_mask = []\n","    start_positions = []\n","    end_positions = []\n","\n","    for example in dataset:\n","        context = example['context']\n","        question = example['question']\n","        answer = example['answers']\n","\n","        inputs = tokenizer(\n","            question,\n","            context,\n","            padding='max_length',\n","            truncation='only_second',\n","            max_length=max_length,\n","            return_overflowing_tokens=True,\n","            return_offsets_mapping=True,\n","            return_tensors='pt'\n","        )\n","\n","        start_idx, end_idx = 0, 0\n","        if answer['answer_start'][0] != -1:\n","            start_idx = inputs.char_to_token(0, answer['answer_start'][0])\n","            end_idx = inputs.char_to_token(0, answer['answer_start'][0] + len(answer['text'][0]))\n","\n","        input_ids.append(inputs.input_ids)\n","        attention_mask.append(inputs.attention_mask)\n","        start_positions.append(start_idx)\n","        end_positions.append(end_idx)\n","\n","    # Перед объединением в тензоры, заполните последовательности до максимальной длины\n","    input_ids = pad_sequence(input_ids, batch_first=True)\n","    attention_mask = pad_sequence(attention_mask, batch_first=True)\n","\n","    # Убедитесь, что start_positions и end_positions не содержат None\n","    start_positions = [0 if x is None else x for x in start_positions]\n","    end_positions = [0 if x is None else x for x in end_positions]\n","\n","    start_positions = torch.tensor(start_positions)\n","    end_positions = torch.tensor(end_positions)\n","\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attention_mask,\n","        'start_positions': start_positions,\n","        'end_positions': end_positions\n","    }"],"metadata":{"id":"bxusMKsR3PmL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets = [\"train\", \"validation\", \"test\"]\n","\n","for dataset_name in datasets:\n","    dataset[dataset_name] = dataset[dataset_name].map(\n","        preprocess_dataset,\n","        batched=True,\n","        remove_columns=dataset[dataset_name].column_names,\n","    )\n","    print(f\"Original {dataset_name} size: {len(dataset[dataset_name])}\")\n","    print(f\"Processed {dataset_name} size: {len(dataset[dataset_name])}\")"],"metadata":{"id":"oQSq0MAqnjgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","# Создаем DataLoader для обработанных датасетов\n","train_loader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(dataset[\"validation\"], batch_size=batch_size)\n","test_loader = DataLoader(dataset[\"test\"], batch_size=batch_size)"],"metadata":{"id":"mgUHKdddnjcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch in train_loader:\n","    input_ids = batch[\"input_ids\"]\n","    attention_mask = batch[\"attention_mask\"]\n","    start_positions = batch[\"start_positions\"]\n","    end_positions = batch[\"end_positions\"]\n","\n","    # Преобразование списков в тензоры\n","    input_ids = torch.stack(input_ids, dim=1).to(device)\n","    attention_mask = torch.stack(attention_mask, dim=1).to(device)\n","    start_positions = torch.tensor(start_positions).to(device)\n","    end_positions = torch.tensor(end_positions).to(device)\n","\n","    print(input_ids.shape, attention_mask.shape, start_positions.shape, end_positions.shape)\n","    break"],"metadata":{"id":"1dJUGzOgnjWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TBd0KUfbnPAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install deberta\n","\n"],"metadata":{"id":"IJeF5HVwnPDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DFuTjf0WnPIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KyDRY_OPjdG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hmXAgSRkjdJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yVfy9WE0jdNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FR9O3OE2jdQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Bx_hA-mdjdTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e24P0c3AjdV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wHD1VBb5jdZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZrDZbBjUjdcC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cRhMK0_djdfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HozvEYRpjdih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0exdPdmgjdmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hH4fIXREjdr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UKqTLH0BjdvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZMe5XHhajdyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WpKUfxJrjd11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_training_data(examples, max_seq_length=356, tokenizer=None):\n","    \"\"\"\n","    Предобрабатывает обучающие данные для модели вопрос-ответ.\n","\n","    Args:\n","        examples (dict): Словарь с обучающими данными, содержащий \"question\", \"context\" и \"answers\".\n","        max_seq_length (int): Максимальная длина последовательности.\n","        tokenizer: Объект токенизатора для преобразования текста в токены.\n","\n","    Returns:\n","        dict: Подготовленные данные, включая input_ids, start_positions и end_positions.\n","    \"\"\"\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    contexts = examples[\"context\"]\n","\n","    # Токенизация вопросов и контекстов\n","    inputs = tokenizer(questions, contexts, max_length=max_seq_length, truncation=True, padding=\"max_length\", return_offsets_mapping=True)\n","\n","    # Извлечение данных\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offsets in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx][0]\n","        start_char, end_char = answer[\"answer_start\"], answer[\"answer_start\"] + len(answer[\"text\"])\n","\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Находим начало и конец контекста\n","        context_start = sequence_ids.index(1)\n","        context_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n","\n","        # Если ответ не полностью в контексте, то меткой будет (0, 0)\n","        if offset_mapping[context_start][0] > start_char or offset_mapping[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Находим начало и конец ответа в контексте\n","            for idx, (start, end) in enumerate(offsets):\n","                if start <= start_char:\n","                    start_positions.append(idx)\n","                if end >= end_char:\n","                    end_positions.append(idx)\n","\n","    # Обновляем входные данные\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","\n","    return inputs"],"metadata":{"id":"cNAgRiNnfUDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Применяем функцию к обучающим данным\n","train_dataset = dataset[\"train\"].map(\n","    lambda examples: preprocess_training_data(examples, max_seq_length=356, tokenizer=tokenizer),\n","    batched=True,\n","    remove_columns=dataset[\"train\"].column_names,\n",")\n","\n","# Выводим количество примеров до и после предобработки\n","print(f\"Train dataset: Original size = {len(dataset['train'])}, Processed size = {len(train_dataset)}\")"],"metadata":{"id":"UAOHAHdsfUKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Применяем функцию к валидационному набору данных\n","valid_dataset = dataset[\"validation\"].map(\n","    preprocess_training_data,  # Используем функцию предобработки\n","    batched=True,                  # Пакетная обработка данных\n","    remove_columns=dataset[\"validation\"].column_names,  # Удаляем столбцы, которые необходимо удалить\n",")\n"],"metadata":{"id":"IKYouG6Jfwjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Выводим количество примеров до и после предобработки\n","print(f\"Validation dataset: Original size = {len(dataset['validation'])}, Processed size = {len(valid_dataset)}\")\n","\n","# Применяем функцию к тестовому набору данных\n","test_dataset = dataset[\"test\"].map(\n","    preprocess_training_data,  # Используем функцию пред"],"metadata":{"id":"j15NAvHjfwgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Egj7G5tzfwaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SCMmvjXQfUOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O_inQKLffUQo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gi6i_er5fUTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z8xj-To1fUWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WBXuxrIcfUZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_training_data(examples, tokenizer, max_sequence_length):\n","    # Extract questions from examples and strip whitespace\n","    questions = [q.strip() for q in examples[\"question\"]]\n","\n","    # Tokenize questions and contexts, and pad to the maximum sequence length\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=max_sequence_length,\n","        padding=\"max_length\",\n","        truncation=True,\n","        return_offsets_mapping=True,\n","        return_tensors=\"pt\"  # Return PyTorch tensors for input_ids, attention_mask, etc.\n","    )\n","\n","    # Extract offset mappings and answers\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        answer = answers[i]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = start_char + len(answer[\"text\"][0])\n","\n","        # Find the token positions that correspond to the answer's start and end\n","        for j, (start, end) in enumerate(offset):\n","            if start_char == start:\n","                start_positions.append(j)\n","            if end_char == end:\n","                end_positions.append(j)\n","\n","    # Convert the lists of start and end positions to tensors\n","    start_positions = torch.tensor(start_positions, dtype=torch.long)\n","    end_positions = torch.tensor(end_positions, dtype=torch.long)\n","\n","    # Update the input data\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","\n","    return inputs"],"metadata":{"id":"TgBfXCloY0yV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to preprocess the datasets\n","def preprocess_datasets(dataset, tokenizer, max_sequence_length):\n","    # Define a function to preprocess a single example\n","    def preprocess_example(example):\n","        return preprocess_training_data(example, tokenizer, max_sequence_length)\n","\n","    # Apply the preprocess_example function to the train, validation, and test datasets\n","    train_dataset = dataset[\"train\"].map(\n","        preprocess_example,\n","        batched=True,\n","        remove_columns=dataset[\"train\"].column_names,\n","    )\n","    print(\"Train dataset - Original:\", len(dataset[\"train\"]), \"Processed:\", len(train_dataset))\n","\n","    valid_dataset = dataset[\"validation\"].map(\n","        preprocess_example,\n","        batched=True,\n","        remove_columns=dataset[\"validation\"].column_names,\n","    )\n","    print(\"Validation dataset - Original:\", len(dataset[\"validation\"]), \"Processed:\", len(valid_dataset))\n","\n","    test_dataset = dataset[\"test\"].map(\n","        preprocess_example,\n","        batched=True,\n","        remove_columns=dataset[\"test\"].column_names,\n","    )\n","    print(\"Test dataset - Original:\", len(dataset[\"test\"]), \"Processed:\", len(test_dataset))\n","\n","    return train_dataset, valid_dataset, test_dataset\n","\n","# Set the maximum sequence length\n","max_sequence_length = 356\n","\n","# Apply the preprocess_datasets function to your dataset\n","train_dataset, valid_dataset, test_dataset = preprocess_datasets(dataset, tokenizer, max_sequence_length)"],"metadata":{"id":"tTUDEvsxZBIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9ozi4BYjZBD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BfEkztAQZA_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NMD050RoZA8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-3oLNtbcZA5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r5hbuQyrZA2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vQwLXcmhZAzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Py2W0wsBZAvA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jKZy5xldZAq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_training_data(examples, tokenizer, max_sequence_length):\n","    # Extract questions from examples and strip whitespace\n","    questions = [q.strip() for q in examples[\"question\"]]\n","\n","    # Tokenize questions and contexts\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=max_sequence_length,\n","        truncation=True,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # Extract offset mappings, sample mappings, answers, and initialize lists for start and end positions\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully in the context, set start and end positions to (0, 0)\n","        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    # Update the input data\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","\n","    return inputs\n"],"metadata":{"id":"UCM-r7P6WU1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q5tTyAZ7WU5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GZaJ1h4xWU69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8k6--we6WU-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context = raw_datasets[\"train\"][0][\"context\"]\n","question = raw_datasets[\"train\"][0][\"question\"]\n","\n","inputs = tokenizer(question, context)\n","tokenizer.decode(inputs[\"input_ids\"])"],"metadata":{"id":"WixyeWJUTsy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 384\n","stride = 70\n","\n","\n","def preprocess_training_examples(examples):\n","    questions = [q.strip() for q in examples['question']]\n","    inputs = tokenizer(\n","        questions,\n","        examples['context'],\n","        max_length=max_length,\n","        truncation='only_second',\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding='max_length',\n","        )\n","\n","    offset_mapping = inputs.pop('offset_mapping')\n","    sample_map = inputs.pop('overflow_to_sample_mapping')\n","    answers = examples['answers']\n","    start_positions = []\n","    end_positions = []\n","\n","    for (i, offset) in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer['answer_start'][0]\n","        end_char = answer['answer_start'][0] + len(answer['text'][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","    # Find the start and end of the context\n","\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","    # If the answer is not fully inside the context, label is (0, 0)\n","\n","        if offset[context_start][0] > end_char \\\n","            or offset[context_end][1] < start_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","\n","      # Otherwise it's the start and end token positions\n","\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs['start_positions'] = start_positions\n","    inputs['end_positions'] = end_positions\n","    return inputs"],"metadata":{"id":"ycYI7p-5UDk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = raw_datasets[\"train\"].map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")"],"metadata":{"id":"9uFNVX98UOYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rRvEJmDDUU9o"},"execution_count":null,"outputs":[]}]}